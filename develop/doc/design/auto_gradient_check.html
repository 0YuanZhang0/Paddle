



<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Auto Gradient Check Design &mdash; PaddlePaddle  documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="../genindex.html"/>
        <link rel="search" title="Search" href="../search.html"/>
    <link rel="top" title="PaddlePaddle  documentation" href="../index.html"/> 
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?b9a314ab40d04d805655aab1deee08ba";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>


  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index_en.html" class="icon icon-home"> PaddlePaddle
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
<nav class="doc-menu-vertical" role="navigation">

<ul>
<li class="toctree-l1"><a class="reference internal" href="../getstarted/index_en.html">GET STARTED</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../getstarted/quickstart_en.html">Quick Start</a></li>
<li class="toctree-l2"><a class="reference internal" href="../getstarted/concepts/use_concepts_en.html">Basic Concept</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../build_and_install/index_en.html">Install and Build</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../build_and_install/pip_install_en.html">Install using pip</a></li>
<li class="toctree-l2"><a class="reference internal" href="../build_and_install/docker_install_en.html">Run in Docker Containers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../build_and_install/build_from_source_en.html">Build from Sources</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../howto/index_en.html">HOW TO</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../howto/cmd_parameter/index_en.html">Set Command-line Parameters</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../howto/cmd_parameter/use_case_en.html">Use Case</a></li>
<li class="toctree-l3"><a class="reference internal" href="../howto/cmd_parameter/arguments_en.html">Argument Outline</a></li>
<li class="toctree-l3"><a class="reference internal" href="../howto/cmd_parameter/detail_introduction_en.html">Detail Description</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../howto/cluster/index_en.html">Distributed Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../howto/cluster/preparations_en.html">Preparations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../howto/cluster/cmd_argument_en.html">Command-line arguments</a></li>
<li class="toctree-l3"><a class="reference internal" href="../howto/cluster/multi_cluster/index_en.html">Use different clusters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../howto/cluster/multi_cluster/fabric_en.html">Fabric</a></li>
<li class="toctree-l4"><a class="reference internal" href="../howto/cluster/multi_cluster/openmpi_en.html">OpenMPI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../howto/cluster/multi_cluster/k8s_en.html">Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../howto/cluster/multi_cluster/k8s_aws_en.html">Kubernetes on AWS</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../howto/rnn/index_en.html">RNN Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../howto/rnn/rnn_config_en.html">RNN Configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../howto/rnn/recurrent_group_en.html">Recurrent Group Tutorial</a></li>
<li class="toctree-l3"><a class="reference internal" href="../howto/rnn/hierarchical_layer_en.html">Layers supporting hierarchical sequence as input</a></li>
<li class="toctree-l3"><a class="reference internal" href="../howto/rnn/hrnn_rnn_api_compare_en.html">API comparision between RNN and hierarchical RNN</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../howto/optimization/gpu_profiling_en.html">Tune GPU Performance</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../dev/index_en.html">Development</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../dev/contribute_to_paddle_en.html">Contribute Code</a></li>
<li class="toctree-l2"><a class="reference internal" href="../dev/write_docs_en.html">Contribute Documentation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../faq/index_en.html">FAQ</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../faq/build_and_install/index_en.html">Install, Build and Unit test</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq/model/index_en.html">Model Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq/parameter/index_en.html">Parameter Setting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq/local/index_en.html">Local Training and Prediction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faq/cluster/index_en.html">Cluster Training and Prediction</a></li>
</ul>
</li>
</ul>

</nav>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../index_en.html">PaddlePaddle</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../index_en.html">Docs</a> &raquo;</li>
      
    <li>Auto Gradient Check Design</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/design/auto_gradient_check.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="auto-gradient-check-design">
<span id="auto-gradient-check-design"></span><h1>Auto Gradient Check Design<a class="headerlink" href="#auto-gradient-check-design" title="Permalink to this headline">¶</a></h1>
</div>
<div class="section" id="background">
<span id="background"></span><h1>Background：<a class="headerlink" href="#background" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li>Generally, it is easy to check whether the forward computation of an Operator is correct or not. However, backpropagation is a notoriously difficult algorithm to debug and get right because of the following challenges:<ol>
<li>The formula for backpropagation formula should be correct according to the forward computation.</li>
<li>The Implementation of the above shoule be correct in CPP.</li>
<li>It is difficult to prepare an unbiased test data.</li>
</ol>
</li>
<li>Auto gradient checking gets a numerical gradient using forward Operator and uses it as a reference for the backward Operator&#8217;s result. It has several advantages:<ol>
<li>Numerical gradient checker only needs the forward operator.</li>
<li>The user only needs to prepare the input data for forward Operator and not worry about the backward Operator.</li>
</ol>
</li>
</ul>
</div>
<div class="section" id="mathematical-theory">
<span id="mathematical-theory"></span><h1>Mathematical Theory<a class="headerlink" href="#mathematical-theory" title="Permalink to this headline">¶</a></h1>
<p>The following documents from Stanford have a detailed explanation of how to compute the numerical gradient and why it is useful.</p>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference external" href="http://deeplearning.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization">Gradient checking and advanced optimization(en)</a></li>
<li class="toctree-l1"><a class="reference external" href="http://ufldl.stanford.edu/wiki/index.php/%E6%A2%AF%E5%BA%A6%E6%A3%80%E9%AA%8C%E4%B8%8E%E9%AB%98%E7%BA%A7%E4%BC%98%E5%8C%96">Gradient checking and advanced optimization(cn)</a></li>
</ul>
</div>
</div>
<div class="section" id="numerical-gradient-implementation">
<span id="numerical-gradient-implementation"></span><h1>Numerical Gradient Implementation<a class="headerlink" href="#numerical-gradient-implementation" title="Permalink to this headline">¶</a></h1>
<div class="section" id="python-interface">
<span id="python-interface"></span><h2>Python Interface<a class="headerlink" href="#python-interface" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_numerical_gradient</span><span class="p">(</span><span class="n">op</span><span class="p">,</span>
                         <span class="n">input_values</span><span class="p">,</span>
                         <span class="n">output_name</span><span class="p">,</span>
                         <span class="n">input_to_check</span><span class="p">,</span>
                         <span class="n">delta</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span>
                         <span class="n">local_scope</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get Numerical Gradient for the input of an operator.</span>

<span class="sd">    :param op: C++ operator instance, could be an network.</span>
<span class="sd">    :param input_values: The input variables. Should be an dictionary, whose key is</span>
<span class="sd">    variable name, and value is a numpy array.</span>
<span class="sd">    :param output_name: The final output variable name.</span>
<span class="sd">    :param input_to_check: The input variable with respect to which the gradient has to be computed.</span>
<span class="sd">    :param delta: The perturbation value for numerical gradient method. The</span>
<span class="sd">    smaller the delta, the more accurate the result. But if the delta is too</span>
<span class="sd">    small, it will suffer from the numerical stability problem.</span>
<span class="sd">    :param local_scope: The local scope used for get_numeric_gradient.</span>
<span class="sd">    :return: The gradient array in numpy format.</span>
<span class="sd">    &quot;&quot;&quot;</span>
</pre></div>
</div>
</div>
<div class="section" id="explanation">
<span id="explanation"></span><h2>Explanation:<a class="headerlink" href="#explanation" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>Why do we need an <code class="docutils literal"><span class="pre">output_name</span></code><ul>
<li>An Operator may have multiple Outputs, one can compute an independent gradient from each Output. So the caller should specify the name of the output variable.</li>
</ul>
</li>
<li>Why do we need <code class="docutils literal"><span class="pre">input_to_check</span></code><ul>
<li>One operator can have multiple inputs. Gradient Op can calculate the gradient of these inputs at the same time. But Numerical Gradient needs to calculate them one by one. So <code class="docutils literal"><span class="pre">get_numeric_gradient</span></code> is designed to calculate the gradient for one input. If you need to compute multiple inputs, you can call <code class="docutils literal"><span class="pre">get_numeric_gradient</span></code> multiple times each with a different input.</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="core-algorithm-implementation">
<span id="core-algorithm-implementation"></span><h2>Core Algorithm Implementation<a class="headerlink" href="#core-algorithm-implementation" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python"><div class="highlight"><pre><span></span>    <span class="c1"># we only compute the gradient of one element a time.</span>
    <span class="c1"># we use a for loop to compute the gradient of each element.</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">tensor_size</span><span class="p">):</span>
        <span class="c1"># get one input element using the index i.</span>
        <span class="n">original</span> <span class="o">=</span> <span class="n">tensor_to_check</span><span class="o">.</span><span class="n">get_float_element</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>

        <span class="c1"># add delta to it, run the forward op and then</span>
        <span class="c1"># get the new value of the result tensor.</span>
        <span class="n">x_pos</span> <span class="o">=</span> <span class="n">original</span> <span class="o">+</span> <span class="n">delta</span>
        <span class="n">tensor_to_check</span><span class="o">.</span><span class="n">set_float_element</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x_pos</span><span class="p">)</span>
        <span class="n">y_pos</span> <span class="o">=</span> <span class="n">get_output</span><span class="p">()</span>

        <span class="c1"># Subtract delta from this element, run the op again</span>
        <span class="c1"># and get the new value of the result tensor.</span>
        <span class="n">x_neg</span> <span class="o">=</span> <span class="n">original</span> <span class="o">-</span> <span class="n">delta</span>
        <span class="n">tensor_to_check</span><span class="o">.</span><span class="n">set_float_element</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x_neg</span><span class="p">)</span>
        <span class="n">y_neg</span> <span class="o">=</span> <span class="n">get_output</span><span class="p">()</span>

        <span class="c1"># restore old value</span>
        <span class="n">tensor_to_check</span><span class="o">.</span><span class="n">set_float_element</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">original</span><span class="p">)</span>

        <span class="c1"># compute the gradient of this element and store</span>
        <span class="c1"># it into a numpy array.</span>
        <span class="n">gradient_flat</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_pos</span> <span class="o">-</span> <span class="n">y_neg</span><span class="p">)</span> <span class="o">/</span> <span class="n">delta</span> <span class="o">/</span> <span class="mi">2</span>

    <span class="c1"># reshape the gradient result to the shape of the source tensor.</span>
    <span class="k">return</span> <span class="n">gradient_flat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tensor_to_check</span><span class="o">.</span><span class="n">get_dims</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="auto-gradient-check-framework">
<span id="auto-gradient-check-framework"></span><h1>Auto Gradient Check Framework<a class="headerlink" href="#auto-gradient-check-framework" title="Permalink to this headline">¶</a></h1>
<p>Each Operator Kernel has three kinds of Gradient:</p>
<ol class="simple">
<li>Numerical gradient</li>
<li>CPU kernel gradient</li>
<li>GPU kernel gradient (if supported by the device)</li>
</ol>
<p>The numerical gradient only relies on the forward Operator, so we use the numerical gradient as the reference value. The gradient checking is performed in the following three steps:</p>
<ol class="simple">
<li>Calculate the numerical gradient</li>
<li>Calculate CPU kernel gradient with the backward Operator and compare it with the numerical gradient.</li>
<li>Calculate GPU kernel gradient with the backward Operator and compare it with the numeric gradient. (if supported)</li>
</ol>
<div class="section" id="python-interface">
<span id="id1"></span><h2>Python Interface<a class="headerlink" href="#python-interface" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python"><div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">check_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                   <span class="n">forward_op</span><span class="p">,</span>
                   <span class="n">input_vars</span><span class="p">,</span>
                   <span class="n">inputs_to_check</span><span class="p">,</span>
                   <span class="n">output_name</span><span class="p">,</span>
                   <span class="n">no_grad_set</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                   <span class="n">only_cpu</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                   <span class="n">max_relative_error</span><span class="o">=</span><span class="mf">0.005</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param forward_op: used to create backward_op</span>
<span class="sd">        :param input_vars: numpy value of input variable. The following</span>
<span class="sd">          computation will use these variables.</span>
<span class="sd">        :param inputs_to_check: the input variable with respect to which the</span>
<span class="sd">          gradient will be computed.</span>
<span class="sd">        :param output_name: The final output variable name.</span>
<span class="sd">        :param max_relative_error: The relative tolerance parameter.</span>
<span class="sd">        :param no_grad_set: used to create backward ops</span>
<span class="sd">        :param only_cpu: only compute and check gradient on cpu kernel.</span>
<span class="sd">        :return:</span>
<span class="sd">        &quot;&quot;&quot;</span>
</pre></div>
</div>
</div>
<div class="section" id="how-to-check-if-two-numpy-arrays-are-close-enough">
<span id="how-to-check-if-two-numpy-arrays-are-close-enough"></span><h2>How to check if two numpy arrays are close enough?<a class="headerlink" href="#how-to-check-if-two-numpy-arrays-are-close-enough" title="Permalink to this headline">¶</a></h2>
<p>if <code class="docutils literal"><span class="pre">abs_numerical_grad</span></code> is nearly zero, then use absolute error for numerical_grad.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">numerical_grad</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">operator_grad</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">scope</span><span class="o">.</span><span class="n">find_var</span><span class="p">(</span><span class="n">grad_var_name</span><span class="p">(</span><span class="n">name</span><span class="p">))</span><span class="o">.</span><span class="n">get_tensor</span><span class="p">())</span>

<span class="n">abs_numerical_grad</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">numerical_grad</span><span class="p">)</span>
<span class="c1"># if abs_numerical_grad is nearly zero, then use abs error for</span>
<span class="c1"># numeric_grad, instead of relative error.</span>
<span class="n">abs_numerical_grad</span><span class="p">[</span><span class="n">abs_numerical_grad</span> <span class="o">&lt;</span> <span class="mf">1e-3</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">diff_mat</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">abs_numerical_grad</span> <span class="o">-</span> <span class="n">operator_grad</span><span class="p">)</span> <span class="o">/</span> <span class="n">abs_numerical_grad</span>
<span class="n">max_diff</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">diff_mat</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="notes">
<span id="notes"></span><h3>Notes：<a class="headerlink" href="#notes" title="Permalink to this headline">¶</a></h3>
<p>The Input data for auto gradient checker should be reasonable to avoid numerical stability problem.</p>
</div>
<div class="section" id="references">
<span id="references"></span><h3>References:<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h3>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference external" href="http://deeplearning.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization">Gradient checking and advanced optimization(en)</a></li>
<li class="toctree-l1"><a class="reference external" href="http://ufldl.stanford.edu/wiki/index.php/%E6%A2%AF%E5%BA%A6%E6%A3%80%E9%AA%8C%E4%B8%8E%E9%AB%98%E7%BA%A7%E4%BC%98%E5%8C%96">Gradient checking and advanced optimization(cn)</a></li>
</ul>
</div>
</div>
</div>
</div>


           </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016, PaddlePaddle developers.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>